# [지연 시간 줄이기](https://platform.claude.com/docs/en/test-and-evaluate/strengthen-guardrails/reduce-latency)

---

지연 시간(latency)은 모델이 프롬프트를 처리하고 출력을 생성하는 데 걸리는 시간을 의미합니다. 지연 시간은 모델의 크기, 프롬프트의 복잡성, 모델과 상호작용 지점을 지원하는 기반 인프라 등 다양한 요인에 의해
영향을 받을 수 있습니다.

> **참고**
>
> 모델이나 프롬프트 제약 없이 잘 작동하는 프롬프트를 먼저 엔지니어링한 다음, 그 이후에 지연 시간 감소 전략을 시도하는 것이 항상 더 좋습니다. 성급하게 지연 시간을 줄이려고 하면 최상의 성능이 어떤 것인지
> 발견하지 못할 수 있습니다.

---

## 지연 시간 측정 방법

지연 시간을 논의할 때 몇 가지 용어와 측정값을 접하게 될 수 있습니다:

- **기본 지연 시간(Baseline latency)**: 입력 및 출력 초당 토큰 수를 고려하지 않고 모델이 프롬프트를 처리하고 응답을 생성하는 데 걸리는 시간입니다. 모델의 속도에 대한 일반적인 아이디어를
  제공합니다.
- **첫 토큰까지의 시간(Time to first token, TTFT)**: 프롬프트가 전송된 시점부터 모델이 응답의 첫 번째 토큰을 생성하는 데 걸리는 시간을 측정하는 지표입니다. 스트리밍을 사용하고(나중에
  자세히 설명) 사용자에게 반응성 있는 경험을 제공하려는 경우 특히 관련이 있습니다.

이러한 용어에 대한 더 심층적인 이해를 원하시면 [용어 사전](https://platform.claude.com/docs/en/about-claude/glossary)을 확인하세요.

---

## 지연 시간을 줄이는 방법

### 1. 적절한 모델 선택

지연 시간을 줄이는 가장 직관적인 방법 중 하나는 사용 사례에 적합한 모델을 선택하는 것입니다. Anthropic은 다양한 기능과 성능 특성을
가진 [다양한 모델](https://platform.claude.com/docs/en/about-claude/models/overview)을 제공합니다. 특정 요구사항을 고려하여 속도와 출력 품질 측면에서 필요에
가장 적합한 모델을 선택하세요.

속도가 중요한 애플리케이션의 경우, **Claude Haiku 4.5**가 높은 지능을 유지하면서 가장 빠른 응답 시간을 제공합니다:

```python
import anthropic

client = anthropic.Anthropic()

# 시간이 중요한 애플리케이션의 경우 Claude Haiku 4.5 사용
message = client.messages.create(
    model="claude-haiku-4-5",
    max_tokens=100,
    messages=[{
        "role": "user",
        "content": "Summarize this customer feedback in 2 sentences: [feedback text]"
    }]
)
```

모델 지표에 대한 자세한 내용은 [모델 개요](https://platform.claude.com/docs/en/about-claude/models/overview) 페이지를 참조하세요.

### 2. 프롬프트 및 출력 길이 최적화

높은 성능을 유지하면서 입력 프롬프트와 예상 출력 모두에서 토큰 수를 최소화하세요. 모델이 처리하고 생성해야 하는 토큰이 적을수록 응답이 더 빨라집니다.

다음은 프롬프트와 출력을 최적화하는 데 도움이 되는 몇 가지 팁입니다:

- **명확하면서도 간결하게**: 프롬프트에서 의도를 명확하고 간결하게 전달하도록 노력하세요. 불필요한 세부사항이나 중복된 정보를
  피하되, [Claude는 사용 사례에 대한 맥락이 부족](../08-prompt-engineering/05-be-clear-and-direct.md)하므로 지시사항이 불분명하면 의도한 논리적 비약을 하지 못할 수
  있다는 점을 유념하세요.
- **더 짧은 응답 요청**: Claude에게 직접 간결하게 작성하도록 요청하세요. Claude 3 제품군 모델은 이전 세대에 비해 개선된 조향성을 가지고 있습니다. Claude가 원하지 않는 길이로 출력하는
  경우, Claude에게 [수다스러움을 억제](../08-prompt-engineering/05-be-clear-and-direct.md)하도록 요청하세요.
  > **팁**: LLM이 단어 대신 [토큰](https://platform.claude.com/docs/en/about-claude/glossary#tokens)을 세는 방식 때문에 정확한 단어 수나 단어 수
  제한을 요청하는 것은 문단이나 문장 수 제한을 요청하는 것만큼 효과적인 전략이 아닙니다.
- **적절한 출력 제한 설정**: `max_tokens` 매개변수를 사용하여 생성된 응답의 최대 길이에 대한 엄격한 제한을 설정하세요. 이렇게 하면 Claude가 지나치게 긴 출력을 생성하는 것을 방지할 수
  있습니다.
  > **참고**: 응답이 `max_tokens` 토큰에 도달하면 응답이 잘릴 수 있으며, 문장이나 단어 중간에서 잘릴 수 있으므로 이는 후처리가 필요할 수 있는 무딘 기술이며 일반적으로 답변이 맨 처음에 나오는
  객관식 또는 단답형 응답에 가장 적합합니다.
- **temperature 실험**: `temperature` [매개변수](https://platform.claude.com/docs/en/api/messages)는 출력의 무작위성을 제어합니다. 낮은 값(예:
  0.2)은 때때로 더 집중되고 짧은 응답으로 이어질 수 있으며, 높은 값(예: 0.8)은 더 다양하지만 잠재적으로 더 긴 출력을 초래할 수 있습니다.

프롬프트 명확성, 출력 품질, 토큰 수 사이에서 적절한 균형을 찾는 것은 약간의 실험이 필요할 수 있습니다.

### 3. 스트리밍 활용

스트리밍은 전체 출력이 완료되기 전에 모델이 응답을 보내기 시작할 수 있도록 하는 기능입니다. 사용자가 모델의 출력을 실시간으로 볼 수 있으므로 애플리케이션의 반응성을 크게 향상시킬 수 있습니다.

스트리밍이 활성화되면 모델의 출력이 도착하는 대로 처리하여 사용자 인터페이스를 업데이트하거나 다른 작업을 병렬로 수행할 수 있습니다. 이는 사용자 경험을 크게 향상시키고 애플리케이션을 더 상호작용적이고 반응적으로
만들 수 있습니다.

사용 사례에 맞는 스트리밍 구현 방법을 알아보려면 [스트리밍 메시지](../02-capabilities/05-streaming-messages.md)를 참조하세요.
